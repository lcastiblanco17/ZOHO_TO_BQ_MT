# Proceso ETL de Zoho CRM a Google BigQuery (Versi칩n Serverless)

Este proyecto implementa un proceso de **Extracci칩n, Transformaci칩n y Carga (ETL)** optimizado para migrar grandes vol칰menes de datos desde **Zoho CRM** hacia **Google BigQuery**. El flujo de trabajo, dise침ado para ejecutarse en **entornos serverless**, opera completamente en memoria para maximizar la eficiencia y evitar el uso de disco.

Utiliza un script de Python que se conecta a la API Bulk Read de Zoho, procesa los datos en memoria con Pandas y los carga directamente a BigQuery usando una cuenta de servicio.

## Diagrama de Flujo del Proceso

Puedes visualizar el flujo completo del proceso en el siguiente diagrama.
*(Nota: El diagrama muestra el flujo conceptual. La implementaci칩n actual ha sido optimizada para un procesamiento 100% en memoria).*
[Ver Diagrama de Flujo en Canva](https://www.canva.com/design/DAGsB7BMznQ/Kukmnp38aW6bU_j-VZzaSA/view)

---

## 游늶 Requerimientos Previos

Aseg칰rate de tener instalado lo siguiente en tu entorno de desarrollo o despliegue.

### Software y Librer칤as

* **Python**: Versi칩n 3.9 o superior.
* **Librer칤as de Python**: Instala las dependencias necesarias desde tu terminal. Se recomienda usar un entorno virtual.
    ```bash
    pip install pandas zohocrmsdk pandas-gbq unidecode
    ```
* **requirements.txt**: Las versiones exactas utilizadas en este proyecto son:
    ```
    # Requires Python >=3.9
    pandas>=2.2.2
    unidecode>=1.4.0
    pandas-gbq>=0.29.1
    zohocrmsdk>=3.1.0
    ```

---

## 丘뙖잺 Configuraci칩n

Antes de ejecutar el proceso, es crucial configurar las credenciales de acceso para Zoho y Google Cloud.

### 1. Credenciales de Zoho CRM

Para interactuar con la API de Zoho, necesitas un `Client ID`, `Client Secret` y un `Refresh Token`.

1.  **Accede a la Consola de API de Zoho**: Ve a [https://api-console.zoho.com/](https://api-console.zoho.com/).
2.  **Crea una Aplicaci칩n**:
    * Haz clic en "Add Client".
    * Selecciona el tipo de cliente **"Self Client"**. Esto es ideal para procesos de servidor a servidor como este ETL.
3.  **Define los Scopes (Permisos)**: Otorga los siguientes permisos para permitir que el script lea los datos necesarios:
    * `ZohoCRM.bulk.READ`
    * `ZohoCRM.modules.ALL`
4.  **Genera las Credenciales**:
    * La consola te proporcionar치 un **Client ID** y un **Client Secret**. Gu치rdalos de forma segura.
    * En la pesta침a "Generate Code", introduce los scopes que definiste y una duraci칩n (ej. 10 minutos). Haz clic en "Create" para obtener un **Grant Token** temporal.
5.  **Obt칠n el Refresh Token**: El `Grant Token` es de corta duraci칩n y se usa una sola vez para generar el `Refresh Token` permanente. Puedes usar un script auxiliar o una herramienta como Postman para intercambiar el Grant Token por tu primer Refresh Token. Este token no expira y ser치 el que uses en la configuraci칩n del ETL.

> 游댐 **Importante**: El script principal del ETL requiere el **Client ID**, **Client Secret** y **Refresh Token** como par치metros de entrada, los cuales idealmente deber칤an ser gestionados como variables de entorno.

### 2. Credenciales de Google Cloud (BigQuery)

El script se autentica con BigQuery usando una **Cuenta de Servicio (Service Account)**.

1.  **Crea un Proyecto**: Si no tienes uno, crea un proyecto en la [Consola de Google Cloud](https://console.cloud.google.com/).
2.  **Habilita la API de BigQuery**: En tu proyecto, busca "BigQuery API" en la Biblioteca de APIs y habil칤tala.
3.  **Crea una Cuenta de Servicio**:
    * Ve a `IAM y administraci칩n` > `Cuentas de servicio`.
    * Haz clic en `+ CREAR CUENTA DE SERVICIO`. Dale un nombre (ej. `zoho-etl-service-account`).
    * Asigna los siguientes roles para permitir que la cuenta gestione datos y trabajos en BigQuery:
        * `Editor de datos de BigQuery`
        * `Usuario de trabajo de BigQuery`
4.  **Genera una Clave JSON**:
    * Una vez creada la cuenta de servicio, ve a la pesta침a `Claves`.
    * Haz clic en `Agregar clave` > `Crear nueva clave`.
    * Selecciona el tipo **JSON** y haz clic en `Crear`.
    * Se descargar치 un archivo JSON. **Ren칩mbralo a `Credentials.json`** y gu치rdalo en el directorio ra칤z de tu proyecto. Este archivo contiene las credenciales que el script usar치 para autenticarse.

---

## 游 Proceso ETL en Memoria

El script est치 optimizado para funcionar sin escribir archivos temporales en el disco.

### 1. Extracci칩n (Extract)

Esta fase se conecta a la **API Bulk Read de Zoho** y descarga los datos directamente a la memoria RAM.

* El script se inicializa con tus credenciales de Zoho.
* Crea trabajos de extracci칩n de forma concurrente usando hilos (`threading`) para cada p치gina de resultados.
* **Extracci칩n Completa vs. Incremental**: El script acepta un par치metro `full_data`.
    * Si `full_data=True`, extrae todos los registros del m칩dulo.
    * Si `full_data=False`, extrae solo los registros de los 칰ltimos 7 d칤as. Debes especificar el nombre de la columna de fecha (`column_date`) a utilizar como filtro (ej. `Modified_Time`).
* Los datos se descargan como archivos `.zip` y se almacenan como objetos `io.BytesIO` en una lista.

### 2. Transformaci칩n (Transform)

En esta fase, los datos brutos se limpian, unifican y preparan para la carga, todo en memoria.

* El script itera sobre la lista de objetos `io.BytesIO`.
* Descomprime cada `.zip` en memoria para acceder al `.csv` que contiene.
* Lee cada `.csv` en un DataFrame de Pandas.
* **Limpia los nombres de las columnas** para que sean compatibles con BigQuery: elimina acentos, convierte espacios y caracteres especiales a guiones bajos (`_`).
* Consolida todos los DataFrames en uno solo.

### 3. Carga (Load)

La fase final toma el DataFrame consolidado y lo sube a BigQuery.

* El script utiliza la librer칤a `pandas-gbq`.
* Se autentica usando el archivo `Credentials.json` de la cuenta de servicio.
* Carga el DataFrame final directamente a la tabla y dataset especificados en BigQuery.
* La carga se realiza con la opci칩n `if_exists='replace'`, que reemplaza la tabla de destino. Esto puede ser cambiado a `append` para cargas incrementales.
